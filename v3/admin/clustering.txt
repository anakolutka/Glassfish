Thoughts about clustering from admin perspective
- Kedar Mhaswade (km@dev.java.net)
-------------------------------------------------------------------------------
Table of Contents:
1. Introduction
2. Detailed Discussion
3. Admin Commands
4. Open Issues

Scope: GlassFish v3's first release with clustering support.

1. Introduction

This document discusses approaches to clustering GlassFish v3 from an admin
standpoint. The topic is vast and there's lot of ground to cover. This document
only covers the specific portions of installation of bits, synchronization of
server configuration and application data, how to make node agent an optional
component and restarting server instances and clusters. 

Note that compatibility with GlassFish v2 is important.
It is possible that first release of GlassFish 3.1 has no "node agent", but
it should be treated as a component that might come in later if there is need.

2. Detailed Discussion of Architecture Components

2.1 High-level Components

As far as the overall architecture is concerned, there are no changes proposed
for this release, other than the possibility that node agent is made optional.
Thus, there is a single admin-server for the entire cluster. The individual 
instances that are part of same or different "clusters" run on same or different
nodes. A domain corresponds to a running process called Domain Admin Server and
different domains can share the same installation. A domain's file system is
termed the "central repository" and it is the one that should be protected
at all costs. Various nodes make copies of a subset of this structure. This is
because not all application/configuration applies to every cluster/node. A node
replicates only the portion that it needs. DAS is the single entry point into
administration and it is a single point of failure.

Following things are (still) not planned:
- DAS failover. Several customers have asked for this, and it may be possible
  to do this later if we did the infrastructure right. 
- An efficient configuration backup/restore solution.

Following changes can be considered:
- Monitoring of server instances can be done without going through DAS. In
  GlassFish v2, a high-overhead cascading solution was present to
  cascade the MBeans from server instances onto DAS. We may rethink that
  solution. At the same time, it may be desirable to let sysadmins monitor
  a particular cluster instance individually.

2.2 Installation of Bits

In GlassFish v2, every machine (node) that participates in clustering needs to
be managed separately as far as product installation is concerned. This is
probably manageable for small number of nodes, but when the cluster size grows,
it becomes unmanabeable. It is highly desirable that we build some form of
provisioning into the core clustering solution. It is possible that we use
secure communication assuming the existence of "sshd" on the target machines.
This form of provisioning is limited to GlassFish only and is not aimed at
system-wide provisioning. Thus, if sshd is available we can do certain things
remotely without having a dedicated process like node agent.

The advantages of doing this are:
1. System administrators need to manage only single machine called the 
   admin-server machine (or the DAS machine). Thus, the update-center
   integration at the DAS machine can be leveraged to update and upgrade
   the GlassFish components and DAS ensures that all the other nodes
   are in sync with this machine. There is no need to run update-center
   client on individual nodes.

2. It enables "finding" nodes from a single location, i.e. DAS. Thus
   after a node is made available in the network, the entire configuration
   of the cluster can be managed from DAS machine using admin tools.

There is a name-space issue here and it is documented as the Open Issue [1].

2.3 Data Synchronization

The application server data is of three types:
a)- server software (modules and libraries)
b)- server configuration (static files)
c)- application data and configuration (static files)

In order to support 2.1, we should be able to synchronize a). To support cluster
management, we have to support synchronization of b) and c). This section talks
about b) and c) but similar discussion applies to a) as well.

All the clients (server instances) and the DAS have to agree on the paths. 

The domain's folder looks like the following:

(| implies a file, || implies a folder)

||---- domain-name
     ||---- config (config files common to all servers including the DAS)
                 |---- domain.xml
                 |---- logging.properties
                 |---- default-web.xml
                 |---- server.policy
                 |---- <ALL OTHER CONFIG FILES COMMON TO ALL INSTANCES)
                 ||---- <server/cluster-name>-config (cluster/server-specific data
                                         copied to instance's config folder)
     ||---- applications 
                 ||---- <application-name>
                             | ---- <application-specific-paths>
     ||---- lib (libraries common to all servers including the DAS)
     ||---- docroot (the default web-container docroot, files are copied to
                     instance's docroot)

2.3.1 Details of the synchronization algorithm on server startup:

This simplistic synchronization algorithm is based on file's modtime and the
java.io.File.setLastModifiedTime(long time) API. For the first version, DAS
does not keep any delta-information as far as modtimes are concerned. All it
has is a specific modtime on all the files it manages. DAS does no calculations
so that a "difference" between two modtimes is presented in terms of a patch
(traditional Unix term) that can be applied at the client's file system.
The API however, is kept independent of actual criterion applied. A file's
(or folder's) modtime is the criterion we use for this release, but it is
possible that a better, more suitable criterion is chosen for subsequent
releases. It's intended that the criterion is configurable, though it is
not a must for this release.

More often than not, a folder (e.g. config folder, application folder etc.),
rather than a file is sync'ed. However, the DAS does not "manage" a folder
directly. It manages "files" in a folder. Changing the modtime of a file under
DAS's control does not automatically change the modtime of the parent folder.
Thus, configuration operations on the DAS should ensure that when a particular
config file (e.g. domain.xml) is changed, the parent folder's modtime is also
modified. In other words, modtime of any file in the domain at the central
repository is its synchronization signature.

Another important aspect of the algorithm is that by default, contents of the
entire folder (recursive traversal) are sent when requested. Standard compression
schemes are employed when sending the contents.

As you probably know, we "upload" the archive (.jar/.war/.ear) when we do the
archive deployment to the DAS. Till now, this archive was "uploaded" to a
temporary location, which means it was thrown away. Going ahead we will be
retaining this archive because the exploded view of an application is a
"true extraction" of its archive. When the archive is not available, one will
be created (e.g. when asadmin deploy --upload=false myapp.war). This is a 
simple optimization that we must do.

Step 0: Initiation of the synchronization request

0.0: A clustered/non-clustered GlassFish server is DAS's sync client.
     Upon server startup, if the DAS is not available, the server starts with
     whatever "view" of the central repository it has. If DAS is available,
     the server requests the folders/files in the order specified in Step 1.
     At each step, the actual data transfer occurs only when the modtimes
     on requested files/folders differ.

     It is possible that server sends a "get-modtime-request" for a list of
     paths beforehand, as a way to optimize the number of requests. Thus, server
     might send its request as:
     Get-Modtimes(config, config/server1-config, applications/hello-world,
     applications/blog, lib/mysql.jar, docroot/*.jpg)
     (Note that glob-style wildcarding is allowed).

     There is also a provision of "exclude-list" while requesting content. This,
     is useful in cases where you require all the contents of a folder except
     a few that client would list (because for example, those items have the
     same modtimes). The request would look like:
     Get(docroot, excl={images/logo.png, images/big*.png})
     (Note that glob-style wildcarding is allowed).
   
     Actual API will cover all these methods.

Step 1: Following is the order in which files/folders are requested.

1.0: The config folder.
1.1: The server specific config folder. This is in config/<server-name>-config
     folder at the central repository. At the server's cache repository, the
     contents of this are placed in "config" folder directly. For example,
     domain-folder/config/server1-config/init.conf goes to 
     instance-folder/config.
1.2: The application folder for each application deployed to that server.
     Server examines the domain.xml and requests the following for each
     deployed application:
     -- application data (i.e. applications/<app-name> folder)
     -- data generated during deployment process (this is TBD, based on
        what we do with generated folder).
1.3: The libraries (lib) folder. This is where database drivers are placed.
1.4: The web-container docroot. Since the docroot is expected to have lot of
     static content, the server sends a request for modtimes of specific
     files to DAS and then does an actual request for contents.

Step 2: Setting the sync modtimes.

After the contents are sync'ed, the modtimes of sync'ed items on DAS and
server must be identical.

It is possible that DAS balances the load of file transfer to other
instances in the same domain. To support the 
distributed data transfer like this, following things should happen:
- All servers in cluster (i.e. the non-admin-server instances as well) should
  be capable of serving the contents from their local repositories.
- A communication channel needs to be available. 

It's not expected for this release that we balance DAS's synchronization load.

2.4 Communication (Transport) Layer
2.5 Cold Start of a Server Instance Without Node Agent

A node agent is a process that controls the life cycle of the server instances.
On each node (machine) we have a node agent process per GlassFish  domain. For
example, if a GlassFish  domain d1 contains a cluster c1 spanning machines m1,
m2 and m3 with three server instances s1, s2, s3 on each of them, we need three
node agents n1, n2 and n3. This is how it was in GlassFish v2.
                   ____________________
                  |    _____________   |
                  |   |             |  |
                  |   |  s1 <--> n1 |  |
                  |   |             |  |
     _______      |   |_____________|  |
    |       |     |        m1          |
    |       |     |    _____________   |
    |  DAS  |     |   |             |  | 
    |   d1  |     |   |  s2 <--> n1 |  |
    |_______|     |   |             |  |  
                  |   |_____________|  |  c1 = {s1, s2, s3}
    d1 contains c1|        m2          |
    d1 contacts   |    _____________   |
    n1, n2, n3    |   |             |  |
                  |   |  s3 <--> n1 |  |
                  |   |             |  |
                  |   |_____________|  |
                  |        m3          |
                  |____________________|

Since n1, n2 and n3 are separate processes themselves, their life cycle needs to
be managed by human administrators. Since we are making node agents optional for
this release, we need an alternate mechanism for situations like:
- start-cluster, which starts all the cluster instances
- start-instance, which starts a clustered or non-clustered server instance

to remotely start the server processes from a DAS process. Hereby, we propose a
solution that depends on the ubiquitous sshd which is both standard and secure.
Thus, when we want to start a process remotely from a DAS process, we contact
the ssh daemon running on a given port (default: 22) on a given machine and ask
it to start the GlassFish server process. If sshd is not running, administrator
needs to manually start the server (by using a local asadmin command start-server)
or manually restart the sshd.

For this to happen, DAS needs to be an ssh client and pure Java libraries are
available for the same in the public domain. In fact, Hudson project uses this
technique to remotely configure the secondary Hudson machines. Thus, the above
picture now looks like:

                   ____________________
                  |    _____________   |
                  |   |             |  |
                  |   |  s1 (sshd)  |  |
                  |   |             |  |
     _______      |   |_____________|  |
    |       |     |        m1          |
    |       |     |    _____________   |
    |  DAS  |     |   |             |  | 
    |   d1  |     |   |  s2 (sshd)  |  |
    |_______|     |   |             |  |  
                  |   |_____________|  |  c1 = {s1, s2, s3}
    d1 contains c1|        m2          |
    d1 contacts   |    _____________   |
    sshd on each  |   |             |  |
    of m1,m2,m3   |   |  s3 (sshd)  |  |
                  |   |             |  |
                  |   |_____________|  |
                  |        m3          |
                  |____________________|

Once a server is started on a machine, it follows the synchronization algorithm
as described in 2.3.1.

2.6 Server Software Upgrade Implications

It is possible that when a cluster size grows, not all nodes in the cluster are
upgraded simultaneously because that means service downtime. In order to
cut the downtime and ensure service availability, the system should be designed
in such a way that for limited period, different nodes can be running slightly
different versions of GlassFish. 

2.7 GlassFish as an SSH server

2.8 Advanced Topics
- Should we have a ZFS-only configuration that dramatically improves the
file system performance?
- Cloud readiness? Subnet boundaries? 

3. New Admin Commands

We propose following new commands for management:
- join-domain -- A local command creates the local file system for the server
                 and registers itself with the DAS (as clustered/non-clustered
                 server).
- leave-domain -- A local command that does opposite of join-domain.
    
- delete-server-config -- A local command that deletes the entry for a server
                          that was deleted when domain was not running.
                          (Equivalent of delete-node-agent-config)
3.1 join-domain

3.2 leave-domain

3.3 delete-server-config


1- Uses the asadmin program options to communicate with DAS when it is running.
2- Optional operand decides the name of the server. Defaults to the value
   returned by InetAddress.getLocalHost().getHostByName().
3- Saves the coordinates of DAS in a local file. Creates the file system off
   which the server can bootstrap itself.

4. Open Issues
4.1-  An installation can be leveraged my multiple domain admin servers. In some
      cases, the installation folder may not be owned (file system permissions)
      by the user who owns the domain. In these cases, the DAS can't generate the
      checksum for paths rooted at installation.

Foot-notes:
